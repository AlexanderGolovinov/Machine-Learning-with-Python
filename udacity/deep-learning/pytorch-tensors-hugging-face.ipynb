{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'As a kid I did think the weapon the murderer wielded was cool, however I was a kid and so I was a bit dumb. Even as a dumb kid though the movies plot was stupid and a bit boring when the killer was not using his light knife to kill people. What amazes me is that the movie has a really solid cast in it. What script did they read when agreeing to be in this movie as it is most assuredly boring and only a means to show off a light saber on a very small scale. The plot at times is incomprehensible and the end is totally chaotic. The whole film seems to rotate around aliens and the one weapon. The plot has two kids and some dude having an alien encounter, flash years later and there seems to be a return as it were in the mix. Dead animals and such to be explored and for some reason the one dude gets the weapon of the aliens and proceeds to use it to go on a very light killing spree. Seriously, you just have to wonder why this movie was made, if you are going to have a killer have some good death scenes, if you are going to have alien encounters show more than a weird light vortex thing, and if you are going to have light sabers then call yourself star wars.', 'label': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "As a kid I did think the weapon the murderer wielded was cool, however I was a kid and so I was a bit dumb. Even as a dumb kid though the movies plot was stupid and a bit boring when the killer was not using his light knife to kill people. What amazes me is that the movie has a really solid cast in it. What script did they read when agreeing to be in this movie as it is most assuredly boring and only a means to show off a light saber on a very sm..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the IMDB dataset, which contains movie reviews\n",
    "# and sentiment labels (positive or negative)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Fetch a review from the training set\n",
    "review_number = 43\n",
    "sample_review = dataset[\"train\"][review_number]\n",
    "\n",
    "print(sample_review)\n",
    "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
    "\n",
    "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
    "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
    "# Did they even think to check on the director's credentials...\n",
    "\n",
    "if sample_review[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n",
    "# Sentiment: Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4936, 0.2694, 0.3940],\n",
      "        [0.2104, 0.9430, 0.4029],\n",
      "        [0.1136, 0.5601, 0.7582]])\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Fill in the missing parts labelled <MASK> with the appropriate code to complete the exercise.\n",
    "\n",
    "# Hint: Use torch.cuda.is_available() to check if GPU is available\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the device to be used for the tensor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a tensor on the appropriate device\n",
    "my_tensor = torch.rand(3, 3, device=device)\n",
    "\n",
    "# Print the tensor\n",
    "print(my_tensor)\n",
    "\n",
    "assert my_tensor.device.type in {\"cuda\", \"cpu\"}\n",
    "assert my_tensor.shape == (3, 3)\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    \"\"\"My Multilayer Perceptron (MLP)\n",
    "\n",
    "    Specifications:\n",
    "        - Input layer: 784 neurons\n",
    "        - Hidden layer: 128 neurons with ReLU activation\n",
    "        - Output layer: 10 neurons with softmax activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(128, 10)   # Hidden to output layer\n",
    "        self.relu = nn.ReLU()           # ReLU activation for hidden layer\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax for output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input (for MNIST, from 28x28 to 784)\n",
    "        x = x.view(-1, 784) \n",
    "        \n",
    "        # Pass the input to the first layer\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass the result to the final layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "my_mlp = MyMLP()\n",
    "print(my_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work here:\n",
    "\n",
    "\n",
    "# Check the number of inputs\n",
    "assert my_mlp.fc1.in_features == 784\n",
    "\n",
    "# Check the number of outputs\n",
    "assert my_mlp.fc2.out_features == 10\n",
    "\n",
    "# Check the number of nodes in the hidden layer\n",
    "assert my_mlp.fc1.out_features == 128\n",
    "\n",
    "# Check that my_mlp.fc1 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc1, nn.Linear)\n",
    "\n",
    "# Check that my_mlp.fc2 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc2, nn.Linear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
